# Stanford transformative AI safety working group

In the long term, artificial intelligence could "precipitate a transition comparable to (or more significant than) the agricultural or industrial revolution" (from the [Open Philanthropy Project blog](https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1)).  

Such a transition may pose significant technical and societal challenges, but, if navigated well, could be fantastically good.

[Stanford Effective Altruism](http://web.stanford.edu/group/ea/) is organizing this seminar series to facilitate research that makes a positive outcome more likely.

## Background material

If you're interested in learning more, [here is some background material that we recomend](http://shlegeris.com/ai-safety-reading-list).  Also recommended are [80000 Hours podcast episodes](https://80000hours.org/podcast/episodes/) on topics related to transformative AI.  (If you only have time to listen to one of these podcasts, listen to the [interview with Paul Christiano](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/).)

## Schedule

We are fortunate this quarter to be visited by excellent researchers representing many different perspectives on technical transformative AI safety research.  The talks will generally be short (about 30 minutes), and we should have ample time for in-depth discussion.

- [Geoffrey Irving](https://naml.us/) ([OpenAI](https://openai.com/)): "AI Safety at OpenAI" ([slides](irving.pdf), [notes from one attendee](https://docs.google.com/document/d/1SuGK3ibyVVKCkbly7DSTYoaCNHxqjLFHIxFOywT48M0/edit?usp=sharing)), 4/24, 5:30pm, [Varian physics building](https://www.google.com/maps/place/Physics+Department/@37.4285413,-122.1731025,19z/data=!3m1!4b1!4m5!3m4!1s0x808fbb2af1d7cd13:0x4dd96aad40ba907b!8m2!3d37.4285412!4d-122.1725553), 3rd floor, room 355 (email the address below if you need help getting in the building or finding the room)
  - [Podcast interview](https://futureoflife.org/2019/03/06/ai-alignment-through-debate-with-geoffrey-irving/) with Geoffrey
  - Papers that Geoffrey plans to mention:
    - [Concrete problems in AI safety](https://arxiv.org/abs/1606.06565)
    - [Deep reinforcement learning from human preferences](https://arxiv.org/abs/1706.03741)
    - [AI safety via debate](https://arxiv.org/abs/1805.00899)
    - [AI safety needs social scientists](https://distill.pub/2019/safety-needs-social-scientists/)
- [Rohin Shah](https://rohinshah.com/) ([Center for Human-Compatible AI](https://humancompatible.ai/), UC Berkeley), "An Overview of AI Alignment", 5/2, 4pm, [Gates](https://www.google.com/maps/place/Gates+Computer+Science,+353+Serra+Mall,+Stanford,+CA+94305/@37.4299866,-122.175519,17z/data=!3m1!4b1!4m5!3m4!1s0x808fbb2b3f50f727:0xfd9cc3200ee97fda!8m2!3d37.4299866!4d-122.1733303) room 304
  - Podcast interviews overviewing AI alignment research agendas ([part 1](https://futureoflife.org/2019/04/11/an-overview-of-technical-ai-alignment-with-rohin-shah-part-1/)) ([part 2](https://futureoflife.org/2019/04/25/an-overview-of-technical-ai-alignment-with-rohin-shah-part-2/))
  - [Alignment newsletter](http://rohinshah.com/alignment-newsletter/)
  - [Value learning sequence](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc)
- Eric Drexler ([Future of Humanity Institute](https://www.fhi.ox.ac.uk/)), "[Comprehensive AI Services](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)", 5/9, 4pm, location TBD
- [Scott Garrabrant](http://scott.garrabrant.com/) ([Machine Intelligence Research Institute](https://intelligence.org/)), details TBD
- [Jacob Steinhardt](https://www.stat.berkeley.edu/~jsteinhardt/) (UC Berkeley), details TBD

## Contact

Feel free to email [Nate Thomas](https://www.linkedin.com/in/nathaniel-thomas-18603079/) (ncthomas@stanford) if you have any questions.
