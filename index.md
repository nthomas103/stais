# Transformative AI safety working group at Stanford

## Purpose

By "transformative AI", we mean "roughly and conceptually, AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution" (quoting from the [Open Philanthropy Project blog](https://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1) -- see also the more detailed definition #2 in that post).

If this transition indeed occurs and is navigated well, the impact could be extremely positive.  [Stanford Effective Altruism](http://web.stanford.edu/group/ea/) is organizing this seminar series to promote research that will make a positive outcome more likely.

We hope these meetings will connect Stanford researchers interested in transformative AI safety with each other and with other researchers and ideas, leading to new high-impact research results.

## Background reading

If you're interested in learning more, [here](http://shlegeris.com/ai-safety-reading-list) is some background material that we recomend.  Also recommended are [80000 Hours podcast episodes](https://80000hours.org/podcast/episodes/) on topics related to transformative AI.  (If you only have time to listen to one of these podcasts, listen to the [interview with Paul Christiano](https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/).)

## Schedule

We are fortunate this quarter to be visited by excellent speakers representing many different perspectives on technical transformative AI safety research.  The talks will generally be short (about 30 minutes), and there will be ample time for in-depth discussion.

- [Geoffrey Irving](https://naml.us/) ([OpenAI](https://openai.com/)): "AI Safety at OpenAI", 4/24, 5:30pm, [Varian physics building](https://www.google.com/maps/place/Physics+Department/@37.4285413,-122.1731025,19z/data=!3m1!4b1!4m5!3m4!1s0x808fbb2af1d7cd13:0x4dd96aad40ba907b!8m2!3d37.4285412!4d-122.1725553), 3rd floor, room 355
  [Podcast interview](https://futureoflife.org/2019/03/06/ai-alignment-through-debate-with-geoffrey-irving/) with Geoffrey
  Papers that Geoffrey plans to mention:
  - [Concrete problems in AI safety](https://arxiv.org/abs/1606.06565)
  - [Deep reinforcement learning from human preferences](https://arxiv.org/abs/1706.03741)
  - [AI safety via debate](https://arxiv.org/abs/1805.00899)
  - [AI safety needs social scientists](https://distill.pub/2019/safety-needs-social-scientists/)
- [Rohin Shah](https://rohinshah.com/) ([CHAI](https://humancompatible.ai/), UC Berkeley), 5/2, 4pm, location TBD
- Eric Drexler ([Future of Humanity Institute](https://www.fhi.ox.ac.uk/)), "[Comprehensive AI Services](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)", 5/9, 4pm, location TBD
- [Scott Garrabrant](http://scott.garrabrant.com/) ([MIRI](https://intelligence.org/)), details TBD
- [Jacob Steinhardt](https://www.stat.berkeley.edu/~jsteinhardt/) (UC Berkeley), details TBD

## Contact

Feel free to email [Nate Thomas](https://www.linkedin.com/in/nathaniel-thomas-18603079/) (ncthomas@stanford) if you have any questions.
